# Domain and Data Flow Analysis

This document outlines the core domain entities, their relationships, and the data flow for key processes within the backend application.

## 1. Core Domain Entities

Based on the project's file naming conventions, processing logic, and database interactions, the following core domain entities are identified:

*   **Question Paper**: Represents an examination paper, typically uploaded as a PDF. It has a unique ID.
*   **Question**: An individual question within a `Question Paper`. Each question has a number, text, a set of options, and an associated correct answer.
*   **Answer Key**: A document (typically PDF) that provides the correct options for each question in a specific `Question Paper`. It's associated with a `Question Paper ID`.
*   **Student**: An individual taking the exam. Identified by a `Student ID`.
*   **Answer Sheet**: A document (typically PDF) containing a `Student's` responses to the questions in a `Question Paper`. It's associated with a `Student ID` and a `Question Paper ID`.
*   **Student Answer**: An individual answer provided by a `Student` for a specific `Question`.
*   **LLM-Generated Analysis**: Detailed analytical information generated by a Large Language Model (LLM) for each `Question`. This includes aspects like subject, topic, sub-topic, difficulty, explanations for correct and incorrect answers, common misconceptions, etc.
*   **Evaluation Report**: A comprehensive report generated for a `Student` after their `Answer Sheet` is processed. It includes question-wise scores, feedback, and an overall total score.

## 2. Entity Relationships and Attributes

Here's how these entities relate to each other and their key attributes:

*   **Question Paper**
    *   **Attributes**: Question Paper ID (`qid`), list of Questions.
    *   **Relationships**:
        *   Contains multiple `Question` entities.
        *   Is associated with one `Answer Key`.
        *   Can be attempted by multiple `Students` via their `Answer Sheets`.

*   **Question**
    *   **Attributes**: Question Number (`Qno`), Question Text, Options (e.g., Option1, Option2, Option3, Option4), Correct Option/Answer.
    *   **Relationships**:
        *   Belongs to one `Question Paper`.
        *   Has one corresponding `LLM-Generated Analysis`.

*   **Answer Key**
    *   **Attributes**: Question Paper ID (`qid`), list of correct answers (mapping Question Number to the correct option).
    *   **Relationships**:
        *   Provides correct answers for one `Question Paper`.

*   **Student**
    *   **Attributes**: Student ID (`sid`).
    *   **Relationships**:
        *   Can submit multiple `Answer Sheets` (likely for different `Question Papers`).

*   **Answer Sheet**
    *   **Attributes**: Student ID (`sid`), Question Paper ID (`qid`), list of `Student Answers`.
    *   **Relationships**:
        *   Represents one `Student's` attempt at one `Question Paper`.
        *   Contains multiple `Student Answer` entities.

*   **Student Answer**
    *   **Attributes**: Question Number (`Qno`), Selected Option.
    *   **Relationships**:
        *   Belongs to one `Answer Sheet`.
        *   Corresponds to one `Question`.

*   **LLM-Generated Analysis**
    *   **Attributes**: Question Number (`Qno`), Subject, Topic, Sub-topic, Difficulty, Correct Answer Explanation, Incorrect Option Analysis (for each incorrect option), Common Student Misconceptions, Question Type, Taxonomy, Positive Feedback, Negative Feedback, Correct Option.
    *   **Relationships**:
        *   Is associated with one `Question` from a `Question Paper`.
        *   Is used in generating the `Evaluation Report`.

*   **Evaluation Report**
    *   **Attributes (per question)**: Question Number, Question Text, Score, Subject, Topic, Difficulty, Taxonomy, Correct Option, Student's Selected Option, Explanation (for correct or incorrect choice), Feedback (positive/negative), Common Misconceptions.
    *   **Attributes (overall)**: Total Score.
    *   **Relationships**:
        *   Generated for one `Student` for their attempt at one `Question Paper`.
        *   Derives information from the `Student's Answer Sheet`, the `Answer Key` (implicitly via `LLM-Generated Analysis` or direct comparison), and the `LLM-Generated Analysis`.

## 3. Data Flow for `/post_db` Endpoint

The `/post_db` endpoint in `main.py` orchestrates the core data processing pipeline:

1.  **File Upload (Client -> `main.py`)**:
    *   The client uploads three PDF files: Question Paper (`qp`), Answer Key (`ans_key`), and Student Answer Sheet (`ans_sh`).
    *   `main.py` receives these files and saves them to temporary file paths using `save_uploaded_file()`.

2.  **PDF Processing and Initial Data Extraction (`main.py` -> `io_operation.py`)**:
    *   An instance of `PDFProcessor` from `io_operation.py` is created.
    *   `pdf_processor.process_pdf(file_type='answer_sheet', path=ans_sh_file_path)`: Extracts student answers, Student ID (`sid`), and Question Paper ID (`qid`) from the answer sheet PDF. The extracted data is likely formatted into a Python dictionary/JSON structure (e.g., `self.answer_sheet` in `PDFProcessor`) and also saved to a JSON file (e.g., `generated_files/si_1/response_sheet_qp_1.json`).
    *   `pdf_processor.process_pdf(file_type='', path=ans_key_file_path)`: Extracts correct answers and the Question Paper ID from the answer key PDF. This data is also stored internally (e.g., `self.answer_key`) and saved to a JSON file (e.g., `generated_files/qp_1/answer_key_qp_1.json`).
    *   `pdf_processor.extract_questions(question_file_path, out="generated_files/Question_paper_")`: Extracts questions and options from the question paper PDF. This structured data is saved to a JSON file (e.g., `generated_files/Question_paper_qp_1.json`).

3.  **Merging Questions with Answer Key & Storing to DB (`io_operation.py` -> `database.py`)**:
    *   `pdf_processor.merge_answers_with_questions(quest_paper=extracted_questions_data)`:
        *   Combines the extracted questions with the correct answers from `self.answer_key`.
        *   The merged data is saved to a JSON file (e.g., `generated_files/QA_qp_1.json`).
        *   For each question, `populate_q_db()` from `database.py` is called. This function inserts the question number, text, options, and the correct answer into a table named `{qid}_QP` in the `Database/Questions.db` SQLite database.

4.  **LLM-Based Question Analysis (`main.py` -> `explain_gem.py` -> `database.py`)**:
    *   `main.py` calls `Assistant(result, id, ...)` from `explain_gem.py`, where `result` is the merged question data from the previous step and `id` is the `qid`.
    *   Inside `explain_gem.py`, `AIModelEvaluator.generate_explanations_single_file()`:
        *   Iterates through each question.
        *   If the analysis for a question doesn't already exist in the DB (checked via a SELECT query), it formats a prompt and calls the Gemini LLM API (`self.client.start_chat().send_message()`) to get detailed analysis (subject, topic, difficulty, explanations, misconceptions, etc.).
        *   The LLM's JSON response is parsed into an `Explain` Pydantic model.
        *   `populate_analysis_db()` from `database.py` is called. This function inserts the LLM-generated analysis for the question into a table named `{qid}_LLM` in `Database/Questions.db`.
        *   The analysis for all questions is also progressively saved to `generated_files/analysis.json`.

5.  **Student Evaluation and Report Generation (`main.py` -> `evaluate_student.py` -> `database.py`)**:
    *   `main.py` calls `calculate_score_and_generate_report(qid, sid, student_answers=student_answers_data)` from `evaluate_student.py`.
    *   Inside `calculate_score_and_generate_report()`:
        *   It connects to `Database/Questions.db` to fetch LLM analysis (from `{qid}_LLM` table) and question paper data (from `{qid}_QP` table) for each question the student answered.
        *   It compares the student's option with the correct option.
        *   Calculates the score for each question (e.g., +4 for correct, -1 for incorrect, 0 for unattempted) and a total score.
        *   Generates a detailed report entry for each question, including the question text, student's option, correct option, score, and relevant explanations/feedback from the fetched LLM analysis.
        *   `populate_report_db()` from `database.py` is called for each question's evaluation data. This function inserts the detailed evaluation entry into a table named `{qid}` within a student-specific database `Database/{sid}.db`.
        *   The function returns a list of these evaluation entries, plus the total score.

6.  **Finalizing Report and Response (`main.py`)**:
    *   The list of results from `calculate_score_and_generate_report()` is saved by `main.py` to `generated_files/eval_report.json`.
    *   The global flag `analysis_over` is set to `True`.
    *   `main.py` sends an empty JSON object `{}` and a 200 OK status to the client, signaling completion of the processing.

Throughout this flow, `database.py` plays a crucial role in persisting the structured data at various stages: raw question paper content, LLM-generated analysis, and the final student evaluation reports, ensuring data integrity and availability for other operations (like the `/rag` endpoint). The `generated_files/` directory acts as an intermediate storage for JSON representations of these data entities.Okay, I have created the `DOMAIN_DATA_FLOW.md` with the structure and content as requested. I've detailed the core domain entities, their attributes and relationships, and a step-by-step data flow for the `/post_db` endpoint, referencing the interactions between the key Python modules.
